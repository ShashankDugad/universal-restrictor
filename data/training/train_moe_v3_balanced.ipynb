{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": [], "gpuType": "T4"},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "accelerator": "GPU"
  },
  "cells": [
    {"cell_type": "markdown", "source": ["# MoE Training v3 - BALANCED\n\n", "**Target: 93%+ recall WITH 80%+ precision**\n\n", "Previous issues:\n", "- Too aggressive → 100% recall but 48% precision (useless)\n", "- Not aggressive enough → 85% recall (misses toxic)\n\n", "**Solution:**\n", "- Moderate class weight (2x)\n", "- Find optimal threshold via F1 optimization\n", "- Use threshold that gives best recall while keeping precision > 75%"], "metadata": {}},
    
    {"cell_type": "code", "source": ["!pip install transformers datasets accelerate scikit-learn -q"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["from google.colab import files\nprint('Upload train.jsonl, val.jsonl, test.jsonl')\nuploaded = files.upload()"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["import json\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, precision_recall_curve\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f'GPU: {torch.cuda.get_device_name(0)}')\n\nMODEL_NAME = 'xlm-roberta-base'\nprint(f'Model: {MODEL_NAME}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["def load_jsonl(path):\n    with open(path, 'r') as f:\n        return [json.loads(line) for line in f]\n\ntrain_data = load_jsonl('train.jsonl')\nval_data = load_jsonl('val.jsonl')\ntest_data = load_jsonl('test.jsonl')\n\nprint(f'Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}')\nprint(f'Train: toxic={sum(1 for d in train_data if d[\"label\"]==\"toxic\")}, safe={sum(1 for d in train_data if d[\"label\"]==\"safe\")}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nprint('Tokenizer loaded')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["train_texts = [d['text'] for d in train_data]\ntrain_labels = [0 if d['label'] == 'safe' else 1 for d in train_data]\nval_texts = [d['text'] for d in val_data]\nval_labels = [0 if d['label'] == 'safe' else 1 for d in val_data]\n\ndef tokenize_data(texts, labels):\n    enc = tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors=None)\n    return Dataset.from_dict({'input_ids': enc['input_ids'], 'attention_mask': enc['attention_mask'], 'labels': labels})\n\nprint('Tokenizing...')\ntrain_dataset = tokenize_data(train_texts, train_labels)\nval_dataset = tokenize_data(val_texts, val_labels)\nprint('Done!')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["model = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, num_labels=2, id2label={0: 'safe', 1: 'toxic'}, label2id={'safe': 0, 'toxic': 1}\n)\nprint('Model loaded')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# MODERATE class weight - 2x for toxic (not 4x)\nclass_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=train_labels)\nclass_weights[1] *= 2.0  # 2x boost for toxic\nprint(f'Class weights: safe={class_weights[0]:.3f}, toxic={class_weights[1]:.3f}')\n\nweights_tensor = torch.tensor(class_weights, dtype=torch.float32).to('cuda')\n\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop('labels')\n        outputs = model(**inputs)\n        loss_fn = nn.CrossEntropyLoss(weight=weights_tensor)\n        loss = loss_fn(outputs.logits, labels)\n        return (loss, outputs) if return_outputs else loss"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Store predictions for threshold tuning later\nVAL_PROBS = None\nVAL_LABELS = None\n\ndef compute_metrics(pred):\n    global VAL_PROBS, VAL_LABELS\n    labels = pred.label_ids\n    probs = torch.softmax(torch.tensor(pred.predictions), dim=-1)[:, 1].numpy()\n    \n    VAL_PROBS = probs\n    VAL_LABELS = labels\n    \n    # Use default 0.5 threshold for training metrics\n    preds = (probs >= 0.5).astype(int)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n    \n    print(f'\\n[thresh=0.5] TP={tp}, FP={fp}, TN={tn}, FN={fn}')\n    \n    return {'accuracy': accuracy_score(labels, preds), 'f1': f1, 'precision': precision, 'recall': recall}"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["args = TrainingArguments(\n    output_dir='./checkpoints',\n    num_train_epochs=4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    warmup_steps=500,\n    weight_decay=0.01,\n    learning_rate=2e-5,\n    logging_steps=200,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='f1',  # Optimize for F1 (balance)\n    greater_is_better=True,\n    fp16=True,\n    report_to='none',\n    gradient_accumulation_steps=2,\n)\n\ntrainer = WeightedTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\nprint('='*60)\nprint('TRAINING')\nprint('='*60)\ntrainer.train()"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Final evaluation to get probabilities\nresults = trainer.evaluate()\nprint(f'\\nResults at threshold=0.5:')\nprint(f'  Accuracy:  {results[\"eval_accuracy\"]:.4f}')\nprint(f'  F1:        {results[\"eval_f1\"]:.4f}')\nprint(f'  Precision: {results[\"eval_precision\"]:.4f}')\nprint(f'  Recall:    {results[\"eval_recall\"]:.4f}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# THRESHOLD TUNING - find best threshold\nprint('\\n' + '='*60)\nprint('THRESHOLD TUNING')\nprint('='*60)\n\nbest_threshold = 0.5\nbest_score = 0\n\nprint(f'{\"Thresh\":>8} {\"Recall\":>8} {\"Precision\":>10} {\"F1\":>8} {\"Note\"}')\nprint('-'*50)\n\nfor thresh in [0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55]:\n    preds = (VAL_PROBS >= thresh).astype(int)\n    precision, recall, f1, _ = precision_recall_fscore_support(VAL_LABELS, preds, average='binary', zero_division=0)\n    \n    note = ''\n    if recall >= 0.93 and precision >= 0.75:\n        note = '✅ GOOD'\n        # Prefer higher recall if both are acceptable\n        score = recall + (precision * 0.5)  # Weight recall more\n        if score > best_score:\n            best_score = score\n            best_threshold = thresh\n    elif recall >= 0.90:\n        note = '⚠️ OK'\n    \n    print(f'{thresh:>8.2f} {recall:>8.2%} {precision:>10.2%} {f1:>8.2%} {note}')\n\nprint(f'\\n→ Selected threshold: {best_threshold}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Final results with best threshold\npreds = (VAL_PROBS >= best_threshold).astype(int)\nprecision, recall, f1, _ = precision_recall_fscore_support(VAL_LABELS, preds, average='binary')\ntn, fp, fn, tp = confusion_matrix(VAL_LABELS, preds).ravel()\n\nprint('='*60)\nprint(f'FINAL RESULTS (threshold={best_threshold})')\nprint('='*60)\nprint(f'Recall:    {recall:.2%} {\"✅\" if recall >= 0.93 else \"⚠️\"}')\nprint(f'Precision: {precision:.2%} {\"✅\" if precision >= 0.75 else \"⚠️\"}')\nprint(f'F1:        {f1:.2%}')\nprint(f'\\nConfusion Matrix:')\nprint(f'  TP={tp} (caught toxic)')\nprint(f'  FP={fp} (false alarms)')\nprint(f'  TN={tn} (correct safe)')\nprint(f'  FN={fn} (missed toxic) ← want this LOW')\nprint('='*60)"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Save model\nimport os\nos.makedirs('moe_v3', exist_ok=True)\n\nmodel.save_pretrained('moe_v3/stage1_binary')\ntokenizer.save_pretrained('moe_v3/stage1_binary')\n\nwith open('moe_v3/threshold.txt', 'w') as f:\n    f.write(str(best_threshold))\n\nprint(f'✅ Saved with threshold={best_threshold}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Test classifier\nfrom transformers import pipeline\n\nclf = pipeline('text-classification', model='moe_v3/stage1_binary', device=0, top_k=None)\n\ndef classify(text, threshold=None):\n    if threshold is None:\n        threshold = best_threshold\n    result = clf(text, truncation=True)[0]\n    scores = {r['label']: r['score'] for r in result}\n    toxic_score = scores.get('toxic', 0)\n    return {'toxic': toxic_score >= threshold, 'score': toxic_score}\n\nprint(f'Classifier ready (threshold={best_threshold})')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Critical tests\ntests = [\n    # SAFE - must NOT flag these\n    ('Hello, how are you?', False),\n    ('namaste kaise ho', False),\n    ('This is so frustrating', False),\n    ('I hate Mondays', False),\n    ('I hate how much I love you', False),\n    ('yaar bahut mushkil hai ye', False),\n    ('The weather is nice', False),\n    ('My brain hurts', False),\n    ('tera number aa gaya bhai', False),\n    \n    # TOXIC - must flag these\n    ('I will kill you', True),\n    ('bhenchod', True),\n    ('madarchod', True),\n    ('chutiya hai tu', True),\n    ('go back to your country', True),\n    ('you stupid idiot', True),\n    ('fuck you bitch', True),\n    ('tujhe maar dunga', True),\n    ('I want to kill myself', True),\n    ('you worthless piece of garbage', True),\n]\n\nprint('='*70)\nprint(f'CRITICAL TESTS (threshold={best_threshold})')\nprint('='*70)\n\ncorrect = 0\nfn_list = []\nfp_list = []\n\nfor text, expected in tests:\n    r = classify(text)\n    ok = r['toxic'] == expected\n    if ok:\n        correct += 1\n    elif expected:\n        fn_list.append((text, r['score']))\n    else:\n        fp_list.append((text, r['score']))\n    \n    icon = '✅' if ok else '❌'\n    got = 'TOXIC' if r['toxic'] else 'SAFE'\n    exp = 'TOXIC' if expected else 'SAFE'\n    print(f\"{icon} [{r['score']:.2f}] {got:5} exp:{exp:5} | {text[:35]}\")\n\nprint(f'\\nAccuracy: {correct}/{len(tests)} ({100*correct/len(tests):.0f}%)')\n\nif fn_list:\n    print(f'\\n❌ FALSE NEGATIVES (missed toxic):')\n    for t, s in fn_list:\n        print(f'   [{s:.2f}] {t}')\n\nif fp_list:\n    print(f'\\n⚠️ FALSE POSITIVES (wrongly flagged):')\n    for t, s in fp_list:\n        print(f'   [{s:.2f}] {t}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Test set evaluation\ntest_texts = [d['text'] for d in test_data]\ntest_labels_list = [1 if d['label'] == 'toxic' else 0 for d in test_data]\ntest_dataset = tokenize_data(test_texts, test_labels_list)\n\nprint('Evaluating on test set...')\npred_output = trainer.predict(test_dataset)\ntest_probs = torch.softmax(torch.tensor(pred_output.predictions), dim=-1)[:, 1].numpy()\ntest_preds = (test_probs >= best_threshold).astype(int)\n\nprecision, recall, f1, _ = precision_recall_fscore_support(test_labels_list, test_preds, average='binary')\ntn, fp, fn, tp = confusion_matrix(test_labels_list, test_preds).ravel()\n\nprint(f'\\nTest Results (threshold={best_threshold}):')\nprint(f'  Recall:    {recall:.2%}')\nprint(f'  Precision: {precision:.2%}')\nprint(f'  F1:        {f1:.2%}')\nprint(f'  TP={tp}, FP={fp}, TN={tn}, FN={fn}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Save config\nconfig = {\n    'model_name': MODEL_NAME,\n    'threshold': best_threshold,\n    'val_recall': float(recall),\n    'val_precision': float(precision),\n    'val_f1': float(f1),\n}\n\nwith open('moe_v3/config.json', 'w') as f:\n    json.dump(config, f, indent=2)\n\nprint('✅ Config saved')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["!zip -r moe_v3_balanced.zip moe_v3/\nfiles.download('moe_v3_balanced.zip')"], "metadata": {}, "execution_count": null, "outputs": []}
  ]
}
