{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": [], "gpuType": "T4"},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "accelerator": "GPU"
  },
  "cells": [
    {"cell_type": "markdown", "source": ["# Universal Restrictor - Model Training\n", "Fine-tune DistilBERT for content moderation"], "metadata": {}},
    {"cell_type": "code", "source": ["!pip install transformers datasets accelerate scikit-learn -q"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["from google.colab import files\nuploaded = files.upload()"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["import json\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\nprint(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["data = []\nwith open('train_unified.jsonl', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nprint(f\"Loaded {len(data)} examples\")\n\nlabel_map = {\"safe\": 0, \"toxic\": 1}\ntexts = [d[\"text\"] for d in data]\nlabels = [label_map[d[\"label\"]] for d in data]\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.1, random_state=42, stratify=labels)\nprint(f\"Train: {len(train_texts)}, Val: {len(val_texts)}\")"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["MODEL_NAME = \"distilbert-base-uncased\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2, id2label={0: \"safe\", 1: \"toxic\"}, label2id={\"safe\": 0, \"toxic\": 1})\nprint(f\"Model loaded: {MODEL_NAME}\")"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["def tokenize(texts, labels):\n    encodings = tokenizer(texts, truncation=True, padding=True, max_length=256)\n    return Dataset.from_dict({\"input_ids\": encodings[\"input_ids\"], \"attention_mask\": encodings[\"attention_mask\"], \"labels\": labels})\n\ntrain_dataset = tokenize(train_texts, train_labels)\nval_dataset = tokenize(val_texts, val_labels)\nprint(f\"Tokenized: Train={len(train_dataset)}, Val={len(val_dataset)}\")"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n    return {\"accuracy\": accuracy_score(labels, preds), \"f1\": f1, \"precision\": precision, \"recall\": recall}"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["training_args = TrainingArguments(\n    output_dir=\"./restrictor-model\",\n    num_train_epochs=3,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_steps=100,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    fp16=True,\n)\n\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, compute_metrics=compute_metrics)\nprint(\"Ready to train!\")"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["trainer.train()"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["results = trainer.evaluate()\nprint(f\"Accuracy: {results['eval_accuracy']:.4f}\")\nprint(f\"F1 Score: {results['eval_f1']:.4f}\")\nprint(f\"Precision: {results['eval_precision']:.4f}\")\nprint(f\"Recall: {results['eval_recall']:.4f}\")"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["model.save_pretrained(\"./restrictor-model-final\")\ntokenizer.save_pretrained(\"./restrictor-model-final\")\nprint(\"Model saved!\")"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["from transformers import pipeline\n\nclassifier = pipeline(\"text-classification\", model=\"./restrictor-model-final\")\n\ntest_texts = [\"Hello, how are you?\", \"I will kill you\", \"The weather is nice\", \"You are worthless\"]\nfor text in test_texts:\n    result = classifier(text)[0]\n    print(f\"{text[:30]:30} -> {result['label']} ({result['score']:.3f})\")"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["!zip -r restrictor-model.zip restrictor-model-final/\nfiles.download('restrictor-model.zip')"], "metadata": {}, "execution_count": null, "outputs": []}
  ]
}
