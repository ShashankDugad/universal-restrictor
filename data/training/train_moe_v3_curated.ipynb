{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": [], "gpuType": "T4"},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "accelerator": "GPU"
  },
  "cells": [
    {"cell_type": "markdown", "source": ["# MoE Training v3 - Curated Dataset (Fixed)\n\n", "**Fixes**:\n", "- Cleaned 670 mislabeled samples\n", "- Balanced dataset\n", "- Higher toxic class weight (2.5x)\n", "- Target: 95%+ recall"], "metadata": {}},
    
    {"cell_type": "code", "source": ["!pip install transformers datasets accelerate scikit-learn sentencepiece -q"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["from google.colab import files\nimport os\n\nprint('Upload from data/datasets/curated/final/:')\nprint('  - train.jsonl')\nprint('  - val.jsonl') \nprint('  - test.jsonl')\nprint()\nuploaded = files.upload()"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["import json\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f'GPU: {torch.cuda.get_device_name(0)}')\nprint(f'CUDA: {torch.cuda.is_available()}')\n\nos.makedirs('moe_v3', exist_ok=True)\n\nMODEL_NAME = 'google/muril-base-cased'\nprint(f'Model: {MODEL_NAME}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Load data\ndef load_jsonl(path):\n    data = []\n    with open(path, 'r') as f:\n        for line in f:\n            data.append(json.loads(line))\n    return data\n\ntrain_data = load_jsonl('train.jsonl')\nval_data = load_jsonl('val.jsonl')\ntest_data = load_jsonl('test.jsonl')\n\nprint(f'Train: {len(train_data)}')\nprint(f'Val:   {len(val_data)}')\nprint(f'Test:  {len(test_data)}')\n\nprint('\\nTrain distribution:')\nfor label, count in Counter(d['label'] for d in train_data).items():\n    print(f'  {label}: {count}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nprint(f'Tokenizer loaded')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "markdown", "source": ["## Stage 1: Binary Classifier\n", "**Goal: 95%+ recall on toxic**"], "metadata": {}},
    
    {"cell_type": "code", "source": ["# Prepare binary data\ndef prep_binary(data):\n    return [{'text': d['text'], 'label': 0 if d['label'] == 'safe' else 1} for d in data]\n\ntrain_binary = prep_binary(train_data)\nval_binary = prep_binary(val_data)\n\ntrain_texts = [d['text'] for d in train_binary]\ntrain_labels = [d['label'] for d in train_binary]\nval_texts = [d['text'] for d in val_binary]\nval_labels = [d['label'] for d in val_binary]\n\nprint(f'Train: {len(train_texts)} (toxic: {sum(train_labels)}, safe: {len(train_labels)-sum(train_labels)})')\nprint(f'Val:   {len(val_texts)} (toxic: {sum(val_labels)}, safe: {len(val_labels)-sum(val_labels)})')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["def tokenize_data(texts, labels):\n    encodings = tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors=None)\n    return Dataset.from_dict({'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask'], 'labels': labels})\n\nprint('Tokenizing...')\ntrain_dataset = tokenize_data(train_texts, train_labels)\nval_dataset = tokenize_data(val_texts, val_labels)\nprint(f'Done!')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["binary_model = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, num_labels=2, id2label={0: 'safe', 1: 'toxic'}, label2id={'safe': 0, 'toxic': 1}\n)\nprint('Binary model loaded!')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# AGGRESSIVE class weights for HIGH RECALL\n# We want to catch ALL toxic content, even at cost of some false positives\nclass_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=train_labels)\nclass_weights[1] *= 2.5  # BOOST toxic weight significantly\nprint(f'Class weights: safe={class_weights[0]:.3f}, toxic={class_weights[1]:.3f}')\n\nweights_tensor = torch.tensor(class_weights, dtype=torch.float32).to('cuda')\n\nclass HighRecallTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop('labels')\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss_fn = nn.CrossEntropyLoss(weight=weights_tensor)\n        loss = loss_fn(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    \n    # Confusion matrix\n    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n    print(f'\\nConfusion Matrix: TP={tp}, FP={fp}, TN={tn}, FN={fn}')\n    print(f'False Negatives (missed toxic): {fn}')\n    \n    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["binary_args = TrainingArguments(\n    output_dir='./stage1_checkpoints',\n    num_train_epochs=4,  # More epochs\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    warmup_steps=500,\n    weight_decay=0.01,\n    learning_rate=2e-5,\n    logging_steps=200,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='recall',\n    greater_is_better=True,\n    fp16=True,\n    report_to='none',\n    gradient_accumulation_steps=2,\n)\n\nbinary_trainer = HighRecallTrainer(\n    model=binary_model,\n    args=binary_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\nprint('='*60)\nprint('TRAINING STAGE 1: Binary (toxic vs safe)')\nprint('Target: 95%+ recall')\nprint('='*60)\nbinary_trainer.train()"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["results1 = binary_trainer.evaluate()\nprint(f'\\n{\"=\"*60}')\nprint('STAGE 1 RESULTS')\nprint(f'{\"=\"*60}')\nprint(f'Accuracy:  {results1[\"eval_accuracy\"]:.4f}')\nprint(f'F1:        {results1[\"eval_f1\"]:.4f}')\nprint(f'Precision: {results1[\"eval_precision\"]:.4f}')\nprint(f'Recall:    {results1[\"eval_recall\"]:.4f}')\n\nif results1['eval_recall'] >= 0.95:\n    print('✅ EXCELLENT - 95%+ recall achieved!')\nelif results1['eval_recall'] >= 0.90:\n    print('⚠️ GOOD - but aim for 95%')\nelse:\n    print('❌ NEEDS IMPROVEMENT - recall too low')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["binary_model.save_pretrained('moe_v3/stage1_binary')\ntokenizer.save_pretrained('moe_v3/stage1_binary')\nprint('✅ Stage 1 saved')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "markdown", "source": ["## Stage 2: Category Classifier"], "metadata": {}},
    
    {"cell_type": "code", "source": ["train_toxic = [d for d in train_data if d['label'] == 'toxic']\nval_toxic = [d for d in val_data if d['label'] == 'toxic']\n\ncategories = ['harassment', 'hate_speech', 'violence', 'sexual', 'threat']\nlabel2id_cat = {cat: i for i, cat in enumerate(categories)}\nid2label_cat = {i: cat for i, cat in enumerate(categories)}\n\ndef map_cat(cat):\n    return label2id_cat.get(cat, 0)  # Default to harassment\n\ntrain_cat_texts = [d['text'] for d in train_toxic]\ntrain_cat_labels = [map_cat(d['category']) for d in train_toxic]\nval_cat_texts = [d['text'] for d in val_toxic]\nval_cat_labels = [map_cat(d['category']) for d in val_toxic]\n\nprint(f'Train toxic: {len(train_cat_texts)}')\nprint(f'Val toxic: {len(val_cat_texts)}')\nprint(f'\\nCategories:')\nfor cat, count in Counter(d['category'] for d in train_toxic).most_common():\n    print(f'  {cat}: {count}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["print('Tokenizing Stage 2...')\ntrain_cat_dataset = tokenize_data(train_cat_texts, train_cat_labels)\nval_cat_dataset = tokenize_data(val_cat_texts, val_cat_labels)\nprint('Done!')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["category_model = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, num_labels=len(categories), id2label=id2label_cat, label2id=label2id_cat\n)\nprint(f'Category model loaded ({len(categories)} classes)')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Balance category weights\nunique_labels = np.unique(train_cat_labels)\ncat_weights = compute_class_weight('balanced', classes=unique_labels, y=train_cat_labels)\n\n# Pad weights if needed\nfull_weights = np.ones(len(categories))\nfor i, label in enumerate(unique_labels):\n    full_weights[label] = cat_weights[i]\n\nprint('Category weights:')\nfor i, cat in enumerate(categories):\n    print(f'  {cat}: {full_weights[i]:.3f}')\n\ncat_weights_tensor = torch.tensor(full_weights, dtype=torch.float32).to('cuda')\n\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop('labels')\n        outputs = model(**inputs)\n        loss_fn = nn.CrossEntropyLoss(weight=cat_weights_tensor)\n        loss = loss_fn(outputs.logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\ndef compute_metrics_cat(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    return {'accuracy': accuracy_score(labels, preds), 'f1': f1, 'precision': precision, 'recall': recall}"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["cat_args = TrainingArguments(\n    output_dir='./stage2_checkpoints',\n    num_train_epochs=4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    warmup_steps=200,\n    weight_decay=0.01,\n    learning_rate=2e-5,\n    logging_steps=200,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='f1',\n    fp16=True,\n    report_to='none',\n    gradient_accumulation_steps=2,\n)\n\ncategory_trainer = WeightedTrainer(\n    model=category_model,\n    args=cat_args,\n    train_dataset=train_cat_dataset,\n    eval_dataset=val_cat_dataset,\n    compute_metrics=compute_metrics_cat,\n)\n\nprint('='*60)\nprint('TRAINING STAGE 2: Category Classifier')\nprint('='*60)\ncategory_trainer.train()"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["results2 = category_trainer.evaluate()\nprint(f'\\n{\"=\"*60}')\nprint('STAGE 2 RESULTS')\nprint(f'{\"=\"*60}')\nprint(f'Accuracy:  {results2[\"eval_accuracy\"]:.4f}')\nprint(f'F1:        {results2[\"eval_f1\"]:.4f}')\n\npreds = category_trainer.predict(val_cat_dataset)\npred_labels = preds.predictions.argmax(-1)\nprint('\\nClassification Report:')\nprint(classification_report(val_cat_labels, pred_labels, target_names=categories, zero_division=0))"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["category_model.save_pretrained('moe_v3/stage2_category')\ntokenizer.save_pretrained('moe_v3/stage2_category')\nprint('✅ Stage 2 saved')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "markdown", "source": ["## Test Pipeline"], "metadata": {}},
    
    {"cell_type": "code", "source": ["from transformers import pipeline\n\nstage1 = pipeline('text-classification', model='moe_v3/stage1_binary', device=0)\nstage2 = pipeline('text-classification', model='moe_v3/stage2_category', device=0)\n\ndef classify(text):\n    r1 = stage1(text, truncation=True)[0]\n    if r1['label'] == 'safe':\n        return {'toxic': False, 'category': 'safe', 'conf': r1['score']}\n    r2 = stage2(text, truncation=True)[0]\n    return {'toxic': True, 'category': r2['label'], 'conf': r1['score'] * r2['score']}\n\nprint('Pipeline ready!')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Critical test cases - MUST pass\ntests = [\n    # SAFE\n    ('Hello, how are you?', False),\n    ('The weather is nice today', False),\n    ('namaste kaise ho', False),\n    ('I went back to my country for vacation', False),\n    ('This is so frustrating', False),\n    ('I hate Mondays', False),\n    ('This is really hard', False),\n    ('I hate how much I love you', False),\n    ('My brain hurts', False),\n    ('yaar bahut mushkil hai ye', False),\n    ('tera number aa gaya bhai', False),\n    \n    # TOXIC\n    ('I will kill you', True),\n    ('bhenchod', True),\n    ('madarchod', True),\n    ('chutiya hai tu', True),\n    ('you are worthless garbage', True),\n    ('go back to your country', True),\n    ('all immigrants should be deported', True),\n    ('tera time aa gaya hai tayyar reh', True),\n    ('tujhe maar dunga', True),\n    ('I want to kill myself', True),\n    ('you stupid idiot', True),\n    ('fuck you bitch', True),\n]\n\nprint('='*70)\nprint('CRITICAL TEST CASES')\nprint('='*70)\n\ncorrect = 0\nfalse_negatives = []\nfalse_positives = []\n\nfor text, should_be_toxic in tests:\n    r = classify(text)\n    is_correct = r['toxic'] == should_be_toxic\n    if is_correct:\n        correct += 1\n    else:\n        if should_be_toxic:\n            false_negatives.append(text)\n        else:\n            false_positives.append(text)\n    \n    icon = '✅' if is_correct else '❌'\n    exp = 'TOXIC' if should_be_toxic else 'SAFE'\n    got = 'TOXIC' if r['toxic'] else 'SAFE'\n    print(f\"{icon} [{got:5}] exp:{exp:5} | {text[:40]}\")\n\nprint(f'\\nAccuracy: {correct}/{len(tests)} ({100*correct/len(tests):.0f}%)')\n\nif false_negatives:\n    print(f'\\n❌ FALSE NEGATIVES (missed toxic):')\n    for t in false_negatives:\n        print(f'   - {t}')\n\nif false_positives:\n    print(f'\\n⚠️ FALSE POSITIVES (wrongly flagged safe as toxic):')\n    for t in false_positives:\n        print(f'   - {t}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Test set evaluation\nprint('='*60)\nprint('TEST SET EVALUATION')\nprint('='*60)\n\ntest_texts = [d['text'] for d in test_data]\ntest_labels = [1 if d['label'] == 'toxic' else 0 for d in test_data]\ntest_dataset = tokenize_data(test_texts, test_labels)\n\ntest_results = binary_trainer.evaluate(test_dataset)\nprint(f'Test Accuracy:  {test_results[\"eval_accuracy\"]:.4f}')\nprint(f'Test F1:        {test_results[\"eval_f1\"]:.4f}')\nprint(f'Test Precision: {test_results[\"eval_precision\"]:.4f}')\nprint(f'Test Recall:    {test_results[\"eval_recall\"]:.4f}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Summary\nprint('\\n' + '='*60)\nprint('FINAL SUMMARY')\nprint('='*60)\nprint(f'Model: {MODEL_NAME}')\nprint(f'Dataset: ~26K samples (cleaned & balanced)')\nprint(f'\\nStage 1 (Binary):')\nprint(f'  Val Recall: {results1[\"eval_recall\"]:.2%} {\"✅\" if results1[\"eval_recall\"] >= 0.95 else \"⚠️\"}')\nprint(f'  Val F1:     {results1[\"eval_f1\"]:.2%}')\nprint(f'\\nStage 2 (Category):')\nprint(f'  Val F1:     {results2[\"eval_f1\"]:.2%}')\nprint('='*60)"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Save config\nconfig = {\n    'model_name': MODEL_NAME,\n    'categories': categories,\n    'label2id_cat': label2id_cat,\n    'stage1_results': {'accuracy': results1['eval_accuracy'], 'f1': results1['eval_f1'], 'recall': results1['eval_recall']},\n    'stage2_results': {'accuracy': results2['eval_accuracy'], 'f1': results2['eval_f1']},\n    'test_results': {'accuracy': test_results['eval_accuracy'], 'f1': test_results['eval_f1'], 'recall': test_results['eval_recall']}\n}\n\nwith open('moe_v3/config.json', 'w') as f:\n    json.dump(config, f, indent=2)\nprint('✅ Config saved')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["!zip -r moe_v3.zip moe_v3/\nfiles.download('moe_v3.zip')"], "metadata": {}, "execution_count": null, "outputs": []}
  ]
}
