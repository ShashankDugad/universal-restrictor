{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": [], "gpuType": "T4"},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "accelerator": "GPU"
  },
  "cells": [
    {"cell_type": "markdown", "source": ["# MoE (Mixture of Experts) Training\n", "## Universal Restrictor - Production Model\n\n", "Architecture:\n", "```\n", "Input → Router (8-class) → Expert Model → toxic/safe\n", "```\n\n", "Models to train:\n", "1. Router: Classifies into 8 categories\n", "2. Experts: Binary classifiers per category"], "metadata": {}},
    
    {"cell_type": "code", "source": ["!pip install transformers datasets accelerate scikit-learn -q"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["from google.colab import files\nimport os\n\nprint('Upload ALL these files from data/datasets/moe/:')\nprint('  - router_train.jsonl')\nprint('  - dangerous.jsonl')\nprint('  - harassment.jsonl')\nprint('  - hate_speech.jsonl')\nprint('  - hindi_abuse.jsonl')\nprint('  - safe.jsonl')\nprint('  - self_harm.jsonl')\nprint('  - sexual.jsonl')\nprint('  - violence.jsonl')\nprint()\nuploaded = files.upload()"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["import json\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nimport numpy as np\n\nprint(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\n\n# Create output directory\nos.makedirs('moe_models', exist_ok=True)"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "markdown", "source": ["## Part 1: Train Router Model (8-class)"], "metadata": {}},
    
    {"cell_type": "code", "source": ["# Load router data\nrouter_data = []\nwith open('router_train.jsonl', 'r') as f:\n    for line in f:\n        router_data.append(json.loads(line))\n\nprint(f'Router data: {len(router_data)} examples')\n\n# Get unique categories\ncategories = sorted(set(d['label'] for d in router_data))\nprint(f'Categories: {categories}')\n\n# Create label mapping\nlabel2id = {cat: i for i, cat in enumerate(categories)}\nid2label = {i: cat for i, cat in enumerate(categories)}\nprint(f'Label mapping: {label2id}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Prepare router training data\ntexts = [d['text'] for d in router_data]\nlabels = [label2id[d['label']] for d in router_data]\n\n# Train/val split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    texts, labels, test_size=0.1, random_state=42, stratify=labels\n)\n\nprint(f'Train: {len(train_texts)}, Val: {len(val_texts)}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Load model for router\nMODEL_NAME = 'distilbert-base-uncased'\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nrouter_model = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=len(categories),\n    id2label=id2label,\n    label2id=label2id\n)\n\nprint(f'Router model: {len(categories)} classes')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Tokenize\ndef tokenize(texts, labels):\n    encodings = tokenizer(texts, truncation=True, padding=True, max_length=256)\n    return Dataset.from_dict({\n        'input_ids': encodings['input_ids'],\n        'attention_mask': encodings['attention_mask'],\n        'labels': labels\n    })\n\ntrain_dataset = tokenize(train_texts, train_labels)\nval_dataset = tokenize(val_texts, val_labels)"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Metrics for multi-class\ndef compute_metrics_multiclass(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    return {\n        'accuracy': accuracy_score(labels, preds),\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Train router\nrouter_args = TrainingArguments(\n    output_dir='./router_checkpoints',\n    num_train_epochs=3,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_steps=500,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='f1',\n    fp16=True,\n    report_to='none',\n)\n\nrouter_trainer = Trainer(\n    model=router_model,\n    args=router_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics_multiclass,\n)\n\nprint('Training Router Model...')\nrouter_trainer.train()"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Evaluate router\nrouter_results = router_trainer.evaluate()\nprint(f'\\nRouter Results:')\nprint(f'  Accuracy: {router_results[\"eval_accuracy\"]:.4f}')\nprint(f'  F1: {router_results[\"eval_f1\"]:.4f}')\n\n# Detailed report\npreds = router_trainer.predict(val_dataset)\npred_labels = preds.predictions.argmax(-1)\nprint('\\nClassification Report:')\nprint(classification_report(val_labels, pred_labels, target_names=categories))"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Save router\nrouter_model.save_pretrained('moe_models/router')\ntokenizer.save_pretrained('moe_models/router')\nprint('Router saved!')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "markdown", "source": ["## Part 2: Train Expert Models (Binary Classifiers)"], "metadata": {}},
    
    {"cell_type": "code", "source": ["# Load safe examples for negative samples\nsafe_examples = []\nwith open('safe.jsonl', 'r') as f:\n    for line in f:\n        safe_examples.append(json.loads(line))\n\nprint(f'Safe examples available: {len(safe_examples)}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["import random\n\ndef train_expert(category_name, category_file, safe_pool, model_name='distilbert-base-uncased'):\n    \"\"\"Train a binary expert model for a category.\"\"\"\n    print(f'\\n{\"=\"*60}')\n    print(f'Training Expert: {category_name.upper()}')\n    print(f'{\"=\"*60}')\n    \n    # Load category data\n    toxic_examples = []\n    with open(category_file, 'r') as f:\n        for line in f:\n            toxic_examples.append(json.loads(line))\n    \n    print(f'Toxic examples: {len(toxic_examples)}')\n    \n    # Sample safe examples (1.5x toxic for 40/60 balance)\n    n_safe = min(int(len(toxic_examples) * 1.5), len(safe_pool))\n    safe_sample = random.sample(safe_pool, n_safe)\n    print(f'Safe examples: {n_safe}')\n    \n    # Prepare data\n    texts = [ex['text'] for ex in toxic_examples] + [ex['text'] for ex in safe_sample]\n    labels = [1] * len(toxic_examples) + [0] * len(safe_sample)  # 1=toxic, 0=safe\n    \n    # Split\n    train_texts, val_texts, train_labels, val_labels = train_test_split(\n        texts, labels, test_size=0.1, random_state=42, stratify=labels\n    )\n    \n    # Load model\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=2,\n        id2label={0: 'safe', 1: 'toxic'},\n        label2id={'safe': 0, 'toxic': 1}\n    )\n    \n    # Tokenize\n    def tokenize(texts, labels):\n        encodings = tokenizer(texts, truncation=True, padding=True, max_length=256)\n        return Dataset.from_dict({\n            'input_ids': encodings['input_ids'],\n            'attention_mask': encodings['attention_mask'],\n            'labels': labels\n        })\n    \n    train_dataset = tokenize(train_texts, train_labels)\n    val_dataset = tokenize(val_texts, val_labels)\n    \n    # Metrics\n    def compute_metrics(pred):\n        labels = pred.label_ids\n        preds = pred.predictions.argmax(-1)\n        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n        return {'accuracy': accuracy_score(labels, preds), 'f1': f1, 'precision': precision, 'recall': recall}\n    \n    # Train\n    args = TrainingArguments(\n        output_dir=f'./{category_name}_checkpoints',\n        num_train_epochs=3,\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=64,\n        warmup_steps=200,\n        weight_decay=0.01,\n        logging_steps=200,\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1',\n        fp16=True,\n        report_to='none',\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics,\n    )\n    \n    trainer.train()\n    \n    # Evaluate\n    results = trainer.evaluate()\n    print(f'\\nResults for {category_name}:')\n    print(f'  Accuracy: {results[\"eval_accuracy\"]:.4f}')\n    print(f'  F1: {results[\"eval_f1\"]:.4f}')\n    print(f'  Precision: {results[\"eval_precision\"]:.4f}')\n    print(f'  Recall: {results[\"eval_recall\"]:.4f}')\n    \n    # Save\n    model.save_pretrained(f'moe_models/{category_name}')\n    tokenizer.save_pretrained(f'moe_models/{category_name}')\n    print(f'Saved to moe_models/{category_name}')\n    \n    return results"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Train all experts\nexpert_categories = [\n    ('dangerous', 'dangerous.jsonl'),\n    ('harassment', 'harassment.jsonl'),\n    ('hate_speech', 'hate_speech.jsonl'),\n    ('hindi_abuse', 'hindi_abuse.jsonl'),\n    ('self_harm', 'self_harm.jsonl'),\n    ('sexual', 'sexual.jsonl'),\n    ('violence', 'violence.jsonl'),\n]\n\nall_results = {}\nfor cat_name, cat_file in expert_categories:\n    results = train_expert(cat_name, cat_file, safe_examples)\n    all_results[cat_name] = results"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Summary\nprint('\\n' + '='*60)\nprint('MOE TRAINING COMPLETE - SUMMARY')\nprint('='*60)\nprint(f'{\"Model\":<15} | {\"Accuracy\":>10} | {\"F1\":>10} | {\"Precision\":>10} | {\"Recall\":>10}')\nprint('-'*60)\nprint(f'{\"Router\":<15} | {router_results[\"eval_accuracy\"]:>10.4f} | {router_results[\"eval_f1\"]:>10.4f} | {router_results[\"eval_precision\"]:>10.4f} | {router_results[\"eval_recall\"]:>10.4f}')\nfor cat, results in all_results.items():\n    print(f'{cat:<15} | {results[\"eval_accuracy\"]:>10.4f} | {results[\"eval_f1\"]:>10.4f} | {results[\"eval_precision\"]:>10.4f} | {results[\"eval_recall\"]:>10.4f}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Test the MoE pipeline\nfrom transformers import pipeline\n\n# Load router\nrouter = pipeline('text-classification', model='moe_models/router')\n\n# Load experts\nexperts = {}\nfor cat in ['dangerous', 'harassment', 'hate_speech', 'hindi_abuse', 'self_harm', 'sexual', 'violence']:\n    experts[cat] = pipeline('text-classification', model=f'moe_models/{cat}')\n\ndef moe_classify(text):\n    # Route\n    route_result = router(text)[0]\n    category = route_result['label']\n    route_conf = route_result['score']\n    \n    if category == 'safe':\n        return {'category': 'safe', 'is_toxic': False, 'confidence': route_conf}\n    \n    # Expert classification\n    expert_result = experts[category](text)[0]\n    is_toxic = expert_result['label'] == 'toxic'\n    expert_conf = expert_result['score']\n    \n    return {\n        'category': category,\n        'is_toxic': is_toxic,\n        'route_confidence': route_conf,\n        'expert_confidence': expert_conf,\n        'combined_confidence': route_conf * expert_conf\n    }\n\n# Test cases\ntest_cases = [\n    ('Hello, how are you?', 'safe'),\n    ('I will kill you', 'violence'),\n    ('bhenchod', 'hindi_abuse'),\n    ('madarchod', 'hindi_abuse'),\n    ('how to make a bomb', 'dangerous'),\n    ('you are worthless garbage', 'harassment'),\n    ('go back to your country', 'hate_speech'),\n    ('I want to kill myself', 'self_harm'),\n]\n\nprint('\\n' + '='*70)\nprint('MOE PIPELINE TEST')\nprint('='*70)\nfor text, expected in test_cases:\n    result = moe_classify(text)\n    status = '✅' if (result['category'] == expected or (expected != 'safe' and result.get('is_toxic', False))) else '❌'\n    print(f\"{status} '{text[:40]:<40}' → {result['category']:<15} (expected: {expected})\")"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Zip and download\n!zip -r moe_models.zip moe_models/\nfiles.download('moe_models.zip')"], "metadata": {}, "execution_count": null, "outputs": []}
  ]
}
