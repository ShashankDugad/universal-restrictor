{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": [], "gpuType": "T4"},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "accelerator": "GPU"
  },
  "cells": [
    {"cell_type": "markdown", "source": ["# MoE Training v3 - HIGH RECALL Edition\n\n", "**Target: 95%+ recall**\n\n", "**Changes:**\n", "- Focal Loss (focuses on hard examples)\n", "- 4x toxic class weight\n", "- Threshold tuning (0.3 instead of 0.5)\n", "- XLM-RoBERTa (better multilingual)"], "metadata": {}},
    
    {"cell_type": "code", "source": ["!pip install transformers datasets accelerate scikit-learn -q"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["from google.colab import files\nprint('Upload train.jsonl, val.jsonl, test.jsonl')\nuploaded = files.upload()"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["import json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f'GPU: {torch.cuda.get_device_name(0)}')\n\n# Try XLM-RoBERTa first (better multilingual), fallback to MuRIL\nMODEL_NAME = 'xlm-roberta-base'\nprint(f'Model: {MODEL_NAME}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["def load_jsonl(path):\n    with open(path, 'r') as f:\n        return [json.loads(line) for line in f]\n\ntrain_data = load_jsonl('train.jsonl')\nval_data = load_jsonl('val.jsonl')\ntest_data = load_jsonl('test.jsonl')\n\nprint(f'Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}')\nprint(f'Train toxic: {sum(1 for d in train_data if d[\"label\"]==\"toxic\")}')\nprint(f'Train safe: {sum(1 for d in train_data if d[\"label\"]==\"safe\")}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nprint('Tokenizer loaded')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Prepare data\ntrain_texts = [d['text'] for d in train_data]\ntrain_labels = [0 if d['label'] == 'safe' else 1 for d in train_data]\nval_texts = [d['text'] for d in val_data]\nval_labels = [0 if d['label'] == 'safe' else 1 for d in val_data]\n\ndef tokenize_data(texts, labels):\n    enc = tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors=None)\n    return Dataset.from_dict({'input_ids': enc['input_ids'], 'attention_mask': enc['attention_mask'], 'labels': labels})\n\nprint('Tokenizing...')\ntrain_dataset = tokenize_data(train_texts, train_labels)\nval_dataset = tokenize_data(val_texts, val_labels)\nprint('Done!')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["model = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, num_labels=2, id2label={0: 'safe', 1: 'toxic'}, label2id={'safe': 0, 'toxic': 1}\n)\nprint('Model loaded')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# FOCAL LOSS - focuses on hard examples\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0):\n        super().__init__()\n        self.alpha = alpha  # Class weights\n        self.gamma = gamma  # Focusing parameter\n    \n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')\n        pt = torch.exp(-ce_loss)  # Probability of correct class\n        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n        return focal_loss.mean()\n\n# Very aggressive class weights: 4x for toxic\nclass_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=train_labels)\nclass_weights[1] *= 4.0  # 4x boost for toxic\nprint(f'Class weights: safe={class_weights[0]:.3f}, toxic={class_weights[1]:.3f}')\n\nweights_tensor = torch.tensor(class_weights, dtype=torch.float32).to('cuda')\nfocal_loss = FocalLoss(alpha=weights_tensor, gamma=2.0)\n\nclass FocalTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop('labels')\n        outputs = model(**inputs)\n        loss = focal_loss(outputs.logits, labels)\n        return (loss, outputs) if return_outputs else loss"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Custom metrics with threshold tuning\nBEST_THRESHOLD = 0.5\n\ndef compute_metrics(pred):\n    global BEST_THRESHOLD\n    labels = pred.label_ids\n    probs = torch.softmax(torch.tensor(pred.predictions), dim=-1)[:, 1].numpy()\n    \n    # Try different thresholds\n    best_recall = 0\n    best_thresh = 0.5\n    for thresh in [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n        preds = (probs >= thresh).astype(int)\n        _, recall, _, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n        if recall > best_recall:\n            best_recall = recall\n            best_thresh = thresh\n    \n    BEST_THRESHOLD = best_thresh\n    preds = (probs >= best_thresh).astype(int)\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n    \n    print(f'\\nThreshold: {best_thresh}, TP={tp}, FP={fp}, TN={tn}, FN={fn}')\n    \n    return {'accuracy': accuracy_score(labels, preds), 'f1': f1, 'precision': precision, 'recall': recall, 'threshold': best_thresh}"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["args = TrainingArguments(\n    output_dir='./checkpoints',\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    warmup_steps=500,\n    weight_decay=0.01,\n    learning_rate=2e-5,\n    logging_steps=200,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='recall',\n    greater_is_better=True,\n    fp16=True,\n    report_to='none',\n    gradient_accumulation_steps=2,\n)\n\ntrainer = FocalTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\nprint('='*60)\nprint('TRAINING - Target: 95%+ Recall')\nprint('='*60)\ntrainer.train()"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["results = trainer.evaluate()\nprint(f'\\n{\"=\"*60}')\nprint('RESULTS')\nprint(f'{\"=\"*60}')\nprint(f'Accuracy:  {results[\"eval_accuracy\"]:.4f}')\nprint(f'F1:        {results[\"eval_f1\"]:.4f}')\nprint(f'Precision: {results[\"eval_precision\"]:.4f}')\nprint(f'Recall:    {results[\"eval_recall\"]:.4f}')\nprint(f'Threshold: {results.get(\"eval_threshold\", BEST_THRESHOLD)}')\n\nif results['eval_recall'] >= 0.95:\n    print('\\n✅ EXCELLENT - 95%+ recall!')\nelif results['eval_recall'] >= 0.90:\n    print('\\n⚠️ GOOD - close to target')\nelse:\n    print('\\n❌ Needs more work')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Save with threshold info\nimport os\nos.makedirs('moe_v3', exist_ok=True)\n\nmodel.save_pretrained('moe_v3/stage1_binary')\ntokenizer.save_pretrained('moe_v3/stage1_binary')\n\n# Save threshold\nwith open('moe_v3/threshold.txt', 'w') as f:\n    f.write(str(BEST_THRESHOLD))\n\nprint(f'✅ Saved with threshold={BEST_THRESHOLD}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Test with custom threshold\nfrom transformers import pipeline\n\nclf = pipeline('text-classification', model='moe_v3/stage1_binary', device=0, top_k=None)\n\ndef classify(text, threshold=None):\n    if threshold is None:\n        threshold = BEST_THRESHOLD\n    result = clf(text, truncation=True)[0]\n    scores = {r['label']: r['score'] for r in result}\n    toxic_score = scores.get('toxic', 0)\n    return {'toxic': toxic_score >= threshold, 'score': toxic_score, 'threshold': threshold}\n\nprint(f'Classifier ready (threshold={BEST_THRESHOLD})')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Critical tests\ntests = [\n    ('Hello, how are you?', False),\n    ('namaste kaise ho', False),\n    ('This is so frustrating', False),\n    ('I hate Mondays', False),\n    ('I hate how much I love you', False),\n    ('yaar bahut mushkil hai ye', False),\n    ('I will kill you', True),\n    ('bhenchod', True),\n    ('madarchod', True),\n    ('chutiya hai tu', True),\n    ('go back to your country', True),\n    ('you stupid idiot', True),\n    ('fuck you bitch', True),\n    ('tujhe maar dunga', True),\n]\n\nprint('='*70)\nprint(f'TESTS (threshold={BEST_THRESHOLD})')\nprint('='*70)\n\ncorrect = 0\nfn = []\nfp = []\n\nfor text, expected in tests:\n    r = classify(text)\n    ok = r['toxic'] == expected\n    if ok:\n        correct += 1\n    elif expected:\n        fn.append((text, r['score']))\n    else:\n        fp.append((text, r['score']))\n    \n    icon = '✅' if ok else '❌'\n    print(f\"{icon} [{r['score']:.2f}] exp:{'TOXIC' if expected else 'SAFE':5} | {text[:40]}\")\n\nprint(f'\\nAccuracy: {correct}/{len(tests)}')\n\nif fn:\n    print(f'\\n❌ FALSE NEGATIVES (missed toxic):')\n    for t, s in fn:\n        print(f'   [{s:.2f}] {t}')\n\nif fp:\n    print(f'\\n⚠️ FALSE POSITIVES:')\n    for t, s in fp:\n        print(f'   [{s:.2f}] {t}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Test on held-out test set\ntest_texts = [d['text'] for d in test_data]\ntest_labels = [1 if d['label'] == 'toxic' else 0 for d in test_data]\n\nprint('Evaluating on test set...')\ntest_dataset = tokenize_data(test_texts, test_labels)\ntest_results = trainer.evaluate(test_dataset)\n\nprint(f'\\nTest Results:')\nprint(f'  Recall:    {test_results[\"eval_recall\"]:.4f}')\nprint(f'  Precision: {test_results[\"eval_precision\"]:.4f}')\nprint(f'  F1:        {test_results[\"eval_f1\"]:.4f}')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Save config\nconfig = {\n    'model_name': MODEL_NAME,\n    'threshold': BEST_THRESHOLD,\n    'val_recall': results['eval_recall'],\n    'val_f1': results['eval_f1'],\n    'test_recall': test_results['eval_recall'],\n    'test_f1': test_results['eval_f1']\n}\n\nwith open('moe_v3/config.json', 'w') as f:\n    json.dump(config, f, indent=2)\n\nprint('Config saved')"], "metadata": {}, "execution_count": null, "outputs": []},
    
    {"cell_type": "code", "source": ["# Download\n!zip -r moe_v3_highrecall.zip moe_v3/\nfiles.download('moe_v3_highrecall.zip')"], "metadata": {}, "execution_count": null, "outputs": []}
  ]
}
